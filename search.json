[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistique en grande dimension",
    "section": "",
    "text": "Présentation\nCe tutoriel présente quelques exercices d’application du cours Modèle linéaire en grande dimension. On pourra trouver\n\nles supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/page_perso/grande_dim.html ;\nle tutoriel sans les corrections à l’url https://lrouviere.github.io/TUTO_GRANDE_DIM/\nle tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_GRANDE_DIM/correction.\n\nIl est recommandé d’utiliser mozilla firefox pour lire le tutoriel.\nDes connaissances de base en R et en statistique (modèles de régression) sont nécessaires. Le tutoriel se structure en 4 parties :\n\nFléau de la dimension : identification du problème de la dimension pour le problème de régression ;\nRégression sur composantes : présentation des algorithmes PCR et PLS ;\nRégressions pénalisées: régularisation à l’aide de pénalités de type Ridge/Lasso\nModèle additif : conservation de la structure additive du modèle linéaire mais modélisation non paramétrique des composantes."
  },
  {
    "objectID": "01-intro-grande-dim.html#fléau-de-la-dimension-pour-les-plus-proches-voisins",
    "href": "01-intro-grande-dim.html#fléau-de-la-dimension-pour-les-plus-proches-voisins",
    "title": "1  Les problèmes de la grande dimension",
    "section": "1.1 Fléau de la dimension pour les plus proches voisins",
    "text": "1.1 Fléau de la dimension pour les plus proches voisins\nLa fonction suivante permet de générer un échantillon d’apprentissage et un échantillon test selon le modèle \\[Y=X_1^2+\\dots+X_p^2+\\varepsilon\\] où les \\(X_j\\) sont uniformes i.i.d de loi uniforme sur \\([0,1]\\) et le bruit \\(\\varepsilon\\) suit une loi \\(\\mathcal N(0,0.5^2)\\).\n\nsimu <- function(napp=300,ntest=500,p=3,graine=1234){\n  set.seed(graine)\n  n <- napp+ntest\n  X <- matrix(runif(n*p),ncol=p)\n  Y <- apply(X^2,1,sum)+rnorm(n,sd=0.5)\n  Yapp <- Y[1:napp]\n  Ytest <- Y[-(1:napp)]\n  Xapp <- data.frame(X[1:napp,])\n  Xtest <- data.frame(X[-(1:napp),])\n  return(list(Xapp=Xapp,Yapp=Yapp,Xtest=Xtest,Ytest=Ytest))\n}\ndf <- simu(napp=300,ntest=500,p=3,graine=1234)\n\nLa fonction knn.reg du package FNN permet de construire des estimateurs des \\(k\\) plus proches voisins en régression. On peut par exemple faire du 3 plus proches voisins avec\n\nlibrary(FNN)\nmod3ppv <- knn.reg(train=df$Xapp,y=df$Yapp,k=3)\n\nParmi toutes les sorties proposées par cette fonction on a notamment\n\nmod3ppv$PRESS\n\n[1] 98.98178\n\n\nqui renvoie la somme des carrés des erreurs de prévision par validation croisée Leave-One-Out (LOO). On peut ainsi obtenir l’erreur quadratique moyenne par LOO\n\nmod3ppv$PRESS/max(c(nrow(df$Xapp),1))\n\n[1] 0.3299393\n\n\n\nConstruire la fonction sel.k qui admet en entrée :\n\nune grille de valeurs possibles de plus proches voisins (un vecteur).\nune matrice Xapp de dimension \\(n\\times p\\) qui contient les valeurs variables explicatives.\nun vecteur Yapp de dimension \\(n\\) qui contient les valeurs de la variable à expliquer\n\net qui renvoie en sortie la valeur de \\(k\\) dans la grille qui minimise l’erreur LOO présentée ci-dessus.\n\nsel.k <- function(K_cand=seq(1,50,by=5),Xapp,Yapp){\n  ind <- 1\n  err <- rep(0,length(K_cand))\n  for (k in K_cand){\n    modkppv <- knn.reg(train=Xapp,y=Yapp,k=k)\n    err[ind] <- modkppv$PRESS/max(c(nrow(Xapp),1))\n    ind <- ind+1\n  }\n  return(K_cand[which.min(err)])\n}\n\nUne fois la fonction créée, on peut calculer l’erreur de l’estimateur sélectionné sur un échantillon test avec\n\nk.opt <- sel.k(seq(1,50,by=5),df$Xapp,df$Yapp)\nk.opt\n\n[1] 31\n\nprev <- knn.reg(train=df$Xapp,y=df$Yapp,test=df$Xtest,k=k.opt)$pred\nmean((prev-df$Ytest)^2)\n\n[1] 0.283869\n\n\nOn souhaite comparer les erreurs des règles des \\(k\\) plus proches voisins en fonction de la dimension. On considère 4 dimensions collectées dans le vecteur DIM et la grille de valeurs de \\(k\\) suivantes :\n\nDIM <- c(1,5,10,50)\nK_cand <- seq(1,50,by=5)\n\nPour chaque valeur de dimension répéter \\(B=100\\) fois :\n\nsimuler un échantillon d’apprentissage de taille 300 et test de taille 500\ncalculer la valeur optimale de \\(k\\) dans K_cand grâce à sel.k\ncalculer l’erreur de l’estimateur sélectionné sur un échantillon test.\n\nOn pourra stocker les résultats dans une matrice de dimension \\(B\\times 4\\).\n\nB <- 100\nmat.err <- matrix(0,ncol=length(DIM),nrow=B)\nfor (p in 1:length(DIM)){\n  for (i in 1:B){\n    df <- simu(napp=300,ntest=500,p=DIM[p],graine=1234*p+2*i)\n    k.opt <- sel.k(K_cand,df$Xapp,df$Yapp)\n    prev <- knn.reg(train=df$Xapp,y=df$Yapp,test=df$Xtest,k=k.opt)$pred\n    mat.err[i,p] <- mean((prev-df$Ytest)^2)\n  }\n}\n\nA l’aide d’indicateurs numériques et de boxplots, comparer la distribution des erreurs en fonction de la dimension.\n\ndf <- data.frame(mat.err)\nnom.dim <- paste(\"D\",DIM,sep=\"\")\nnames(df) <- nom.dim\n\n\ndf |> summarise_all(mean)\n\n        D1        D5     D10      D50\n1 0.258003 0.3243574 0.52247 3.191055\n\ndf |> summarise_all(var)\n\n            D1           D5         D10        D50\n1 0.0002556399 0.0005417109 0.001857967 0.06749414\n\n\n\ndf1 <- pivot_longer(df,cols=everything(),names_to=\"dim\",values_to=\"erreur\")\ndf1 <- df1 |> mutate(dim=fct_relevel(dim,nom.dim))\nggplot(df1)+aes(x=dim,y=erreur)+geom_boxplot()\n\n\n\n\nConclure\n\nLes estimateurs sont moins précis lorsque la dimension augmente. C’est le fléau de la dimension."
  },
  {
    "objectID": "01-intro-grande-dim.html#influence-de-la-dimension-dans-le-modèle-linéaire",
    "href": "01-intro-grande-dim.html#influence-de-la-dimension-dans-le-modèle-linéaire",
    "title": "1  Les problèmes de la grande dimension",
    "section": "1.2 Influence de la dimension dans le modèle linéaire",
    "text": "1.2 Influence de la dimension dans le modèle linéaire\nEn vous basant sur l’exercice précédent, proposer une illustration qui peut mettre en évidence la précision d’estimation dans le modèle linéaire en fonction de la dimension. On pourra par exemple considérer le modèle linaire suivant \\[Y=X_1+0X_2+\\dots+0X_p+\\varepsilon\\] et étudier la performance de l’estimateur MCO du coefficient de \\(X_1\\) pour différentes valeurs de \\(p\\). Par exemple avec \\(p\\) dans le vecteur\n\nDIM <- c(0,50,100,200)\n\nLes données pourront être générées avec la fonction suivante\n\nn <- 250\np <- 1000\nX <- matrix(runif(n*p),ncol=p)\nsimu.lin <- function(X,graine){\n  set.seed(graine)\n  Y <- X[,1]+rnorm(nrow(X),sd=0.5)\n  df <- data.frame(Y,X)\n  return(df)\n}\n\n\nOn s’intéresse à la distribution de \\(\\widehat\\beta_1\\) en fonction de la dimension. Pour ce faire, on calcule un grand nombre d’estimateurs de \\(\\widehat\\beta_1\\) pour différentes valeurs de \\(p\\).\n\n\nB <- 500\nmatbeta1 <- matrix(0,nrow=B,ncol=length(DIM))\nfor (i in 1:B){\n  dftot <- simu.lin(X,i+1)\n  for (p in 1:length(DIM)){\n    dfp <- dftot[,(1:(2+DIM[p]))]\n    mod <- lm(Y~.,data=dfp)\n    matbeta1[i,p] <- coef(mod)[2]\n  }\n}\n\n\nOn met en forme les résultats\n\n\ndf <- data.frame(matbeta1)\nnom.dim <- paste(\"D\",DIM,sep=\"\")\nnames(df) <- nom.dim\n\n\nPuis on compare, pour chaque dimension considérée, les distributions de \\(\\widehat\\beta_1\\) :\n\nen étudiant le biais et la variance\n\ndf |> summarise_all(mean)\n\n        D0       D50      D100    D200\n1 0.992891 0.9960811 0.9959025 0.98173\n\ndf |> summarise_all(var)\n\n          D0      D50       D100       D200\n1 0.01266578 0.016072 0.02023046 0.06939837\n\n\nen visualisant la distribution avec un boxplot\n\ndf1 <- gather(df,key=\"dim\",value=\"erreur\")\ndf1 <- df1 |> mutate(dim=fct_relevel(dim,nom.dim))\nggplot(df1)+aes(x=dim,y=erreur)+geom_boxplot()+theme_classic()\n\n\n\n\nOn retrouve bien que la dimension impacte notamment la variance des estimateurs."
  },
  {
    "objectID": "01-intro-grande-dim.html#exercices",
    "href": "01-intro-grande-dim.html#exercices",
    "title": "1  Les problèmes de la grande dimension",
    "section": "1.3 Exercices",
    "text": "1.3 Exercices\n\nExercice 1.1 (Distances entre deux points) Cet exercice est fortement inspiré de @gir15. Soit \\(X^{(1)}=(X_1^{(1)},\\dots,X_p^{(1)})\\) et \\(X^{(2)}=(X_1^{(2)},\\dots,X_p^{(2)})\\) deux variables aléatoires indépendantes de loi uniforme sur l’hypercube \\([0,1]^p\\). Montrer que \\[\\mathbf E[\\|X^{(1)}-X^{(2)}\\|^2]=\\frac{p}{6}\\quad\\text{et}\\quad\\sigma[\\|X^{(1)}-X^{(2)}\\|^2]\\approx 0.2\\sqrt{p}.\\]\n\nSoit \\(U\\) et \\(U^\\prime\\) deux variables aléatoires indépendantes de loi uniforme sur \\([0,1]\\). On a \\[\\mathbf E[\\|X^{(1)}-X^{(2)}\\|^2]=\\sum_{k=1}^p\\mathbf E\\left[\\left(X_k^{(1)}-X_k^{(2)}\\right)\\right]=p\\mathbf E[(U-U^\\prime)^2]=p(2\\mathbf E[U^2]-2\\mathbf E[U]^2)=\\frac{p}{6}\\] car \\(\\mathbf E[U^2]=1/3\\) et \\(\\mathbf E[U]=1/2\\). De même \\[\\sigma[\\|X^{(1)}-X^{(2)}\\|^2]=\\sqrt{\\sum_{k=1}^p\\mathbf V\\left[\\left(X_k^{(1)}-X_k^{(2)}\\right)\\right]}=\\sqrt{p\\mathbf V[(U^\\prime-U)^2]}\\approx 0.2\\sqrt{p}\\] car \\[\\mathbf E\\left[(U^\\prime-U)^4\\right]=\\int_0^1\\int_0^1(x-y)^4\\,\\mathrm{d}x\\mathrm{d}y=\\frac{1}{15}\\] et donc \\(\\mathbf V[(U^\\prime-U)^2]=1/15-1/36\\approx 0.04\\).\n\n\n\nExercice 1.2 (Vitesse de convergence pour l’estimateur à noyau) On considère le modèle de régression \\[Y_i=m(x_i)+\\varepsilon_i,\\quad i=1,\\dots,n\\] où \\(x_1,\\dots,x_n\\in\\mathbb R^d\\) sont déterministes et \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) sont des variables i.i.d. d’espérance nulle et de variance \\(\\sigma^2<+\\infty\\). On désigne par \\(\\|\\,.\\,\\|\\) la norme Euclidienne dans \\(\\mathbb R^d\\). On définit l’estimateur localement constant de \\(m\\) en \\(x\\in\\mathbb R^d\\) par : \\[\\hat m(x)=\\mathop{\\mathrm{argmin}}_{a\\in\\mathbb R}\\sum_{i=1}^n(Y_i-a)^2K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] où \\(h>0\\) et pour \\(u\\in\\mathbb R,K(u)=\\mathbf 1_{[0,1]}(u)\\). On suppose que \\(\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)>0\\).\n\nDonner la forme explicite de \\(\\hat m(x)\\).\n\nEn annulant la dérivée par rapport à \\(a\\), on obtient \\[\\hat m(x)=\\frac{\\sum_{i=1}^nY_iK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\n\nMontrer que \\[\\mathbf V[\\hat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}\\] et \\[\\mathbf E[\\hat m(x)]-m(x)=\\frac{\\sum_{i=1}^n(m(x_i)-m(x))K\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\n\nCes propriétés se déduisent directement en remarquant que \\(\\mathbf V[Y_i]=\\sigma^2\\) et \\(\\mathbf E[Y_i]=m(x_i)\\).\n\nOn suppose maintenant que \\(m\\) est Lipschitzienne de constante \\(L\\), c’est-à-dire que \\(\\forall (x_1,x_2)\\in\\mathbb R^d\\times\\mathbb R^d\\) \\[|m(x_1)-m(x_2)|\\leq L\\|x_1-x_2\\|.\\] Montrer que \\[|\\textrm{biais}[\\hat m(x)]|\\leq Lh.\\]\n\nOn a \\(|m(x_i)-m(x)|\\leq L\\|x_i-x\\|\\). Or \\[K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] est non nul si et seulement si \\(\\|x_i-x\\|\\leq h\\). Donc pour tout \\(i=1,\\dots,n\\) \\[L\\|x_i-x\\|K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\leq Lh K\\left(\\frac{\\|x_i-x\\|}{h}\\right).\\] D’où le résultat.\n\nOn suppose de plus qu’il existe une constante \\(C_1\\) telle que \\[C_1\\leq\\frac{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}{n\\textrm{Vol}(B_h)},\\] où \\(B_h=\\{u\\in\\mathbb R^d:\\|u\\|\\leq h\\}\\) est la boule de rayon \\(h\\) dans \\(\\mathbb R^d\\) et \\(\\textrm{Vol}(A)\\) désigne le volume d’un ensemble \\(A\\subset\\mathbb R^d\\). Montrer que \\[\\mathbf V[\\hat m(x)]\\leq\\frac{C_2\\sigma^2}{nh^d},\\] où \\(C_2\\) est une constante dépendant de \\(C_1\\) et \\(d\\) à préciser.\n\nOn a \\[\\mathbf V[\\hat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}=\\frac{\\sigma^2}{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}.\\] Or \\[\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)\\geq C_1n\\textrm{Vol}(B_h)\\geq C_1\\gamma_dnh^d\\] où \\(\\gamma_d\\) désigne le volume de la boule unité en dimension \\(d\\). On a donc \\[\\mathbf V[\\hat m(x)]\\leq \\frac{\\sigma^2}{C_1\\gamma_dnh^d}.\\]\n\nDéduire des questions précédentes un majorant de l’erreur quadratique moyenne de \\(\\hat m(x)\\).\n\nOn déduit \\[\\mathbf E[(\\hat m(x)-m(x))^2]\\leq L^2h^2+\\frac{C_2\\sigma^2}{nh^d}.\\]\n\nCalculer \\(h_{\\text{opt}}\\) la valeur de \\(h\\) qui minimise ce majorant. Que vaut ce majorant lorsque \\(h=h_{\\text{opt}}\\) ? Comment varie cette vitesse lorsque \\(d\\) augmente ? Interpréter.\n\nSoit \\(M(h)\\) le majorant. On a \\[M(h)^\\prime=2hL^2-\\frac{C_2\\sigma^2d}{n}h^{-d-1}.\\] La dérivée s’annule pour \\[h_{\\text{opt}}=\\frac{2L^2}{C_2\\sigma^2d}n^{-\\frac{1}{d+2}}.\\] Lorsque \\(h=h_{\\text{opt}}\\) l’erreur quadratique vérifie \\[\\mathbf E[(\\hat m(x)-m(x))^2]=\\mathrm{O}\\left(n^{-\\frac{2}{d+2}}\\right).\\]"
  },
  {
    "objectID": "02-reg-comp.html#sélection-de-variables",
    "href": "02-reg-comp.html#sélection-de-variables",
    "title": "2  Régression sur composantes",
    "section": "2.1 Sélection de variables",
    "text": "2.1 Sélection de variables\nOn considère le jeu de données ozone.txt où on cherche à expliquer la concentration maximale en ozone relevée sur une journée (variable maxO3) par d’autres variables essentiellement météorologiques.\n\nozone <- read.table(\"data/ozone.txt\")\nhead(ozone)\n\n         maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v\n20010601    87 15.6 18.5 18.4   4    4    8  0.6946 -1.7101 -0.6946     84\n20010602    82 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000     87\n20010603    92 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209     82\n20010604   114 16.2 19.7 22.5   1    1    0  0.9848  0.3473 -0.1736     92\n20010605    94 17.4 20.5 20.4   8    8    7 -0.5000 -2.9544 -4.3301    114\n20010606    80 17.7 19.8 18.3   6    6    7 -5.6382 -5.0000 -6.0000     94\n          vent pluie\n20010601  Nord   Sec\n20010602  Nord   Sec\n20010603   Est   Sec\n20010604  Nord   Sec\n20010605 Ouest   Sec\n20010606 Ouest Pluie\n\n\n\nAjuster un modèle linéaire avec lm et analyser la pertinence des variables explicatives dans le modèle.\n\nlin.complet <- lm(maxO3~.,data=ozone)\nsummary(lin.complet)\n\n\nCall:\nlm(formula = maxO3 ~ ., data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.814  -8.695  -1.020   7.891  40.046 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 16.26536   15.94398   1.020   0.3102    \nT9           0.03917    1.16496   0.034   0.9732    \nT12          1.97257    1.47570   1.337   0.1844    \nT15          0.45031    1.18707   0.379   0.7053    \nNe9         -2.10975    0.95985  -2.198   0.0303 *  \nNe12        -0.60559    1.42634  -0.425   0.6721    \nNe15        -0.01718    1.03589  -0.017   0.9868    \nVx9          0.48261    0.98762   0.489   0.6262    \nVx12         0.51379    1.24717   0.412   0.6813    \nVx15         0.72662    0.95198   0.763   0.4471    \nmaxO3v       0.34438    0.06699   5.141 1.42e-06 ***\nventNord     0.53956    6.69459   0.081   0.9359    \nventOuest    5.53632    8.24792   0.671   0.5037    \nventSud      5.42028    7.16180   0.757   0.4510    \npluieSec     3.24713    3.48251   0.932   0.3534    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.51 on 97 degrees of freedom\nMultiple R-squared:  0.7686,    Adjusted R-squared:  0.7352 \nF-statistic: 23.01 on 14 and 97 DF,  p-value: < 2.2e-16\n\nanova(lin.complet)\n\nAnalysis of Variance Table\n\nResponse: maxO3\n          Df Sum Sq Mean Sq  F value    Pr(>F)    \nT9         1  43138   43138 205.0160 < 2.2e-16 ***\nT12        1  11125   11125  52.8706 9.165e-11 ***\nT15        1    876     876   4.1619 0.0440614 *  \nNe9        1   3244    3244  15.4170 0.0001613 ***\nNe12       1    232     232   1.1035 0.2961089    \nNe15       1      5       5   0.0248 0.8752847    \nVx9        1   2217    2217  10.5355 0.0016079 ** \nVx12       1      1       1   0.0049 0.9443039    \nVx15       1     67      67   0.3186 0.5737491    \nmaxO3v     1   6460    6460  30.6993 2.584e-07 ***\nvent       3    234      78   0.3709 0.7741473    \npluie      1    183     183   0.8694 0.3534399    \nResiduals 97  20410     210                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nIl semble que quelques variables ne sont pas nécessaires dans le modèle.\n\nExpliquer les sorties de la commande\n\nlibrary(leaps)\nmod.sel <- regsubsets(maxO3~.,data=ozone,nvmax=14)\nsummary(mod.sel)\n\nSubset selection object\nCall: regsubsets.formula(maxO3 ~ ., data = ozone, nvmax = 14)\n14 Variables  (and intercept)\n          Forced in Forced out\nT9            FALSE      FALSE\nT12           FALSE      FALSE\nT15           FALSE      FALSE\nNe9           FALSE      FALSE\nNe12          FALSE      FALSE\nNe15          FALSE      FALSE\nVx9           FALSE      FALSE\nVx12          FALSE      FALSE\nVx15          FALSE      FALSE\nmaxO3v        FALSE      FALSE\nventNord      FALSE      FALSE\nventOuest     FALSE      FALSE\nventSud       FALSE      FALSE\npluieSec      FALSE      FALSE\n1 subsets of each size up to 14\nSelection Algorithm: exhaustive\n          T9  T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 Vx15 maxO3v ventNord ventOuest\n1  ( 1 )  \" \" \"*\" \" \" \" \" \" \"  \" \"  \" \" \" \"  \" \"  \" \"    \" \"      \" \"      \n2  ( 1 )  \" \" \"*\" \" \" \" \" \" \"  \" \"  \" \" \" \"  \" \"  \"*\"    \" \"      \" \"      \n3  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \" \" \" \"  \" \"  \"*\"    \" \"      \" \"      \n4  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \" \"  \"*\"    \" \"      \" \"      \n5  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \" \"  \"*\"    \" \"      \" \"      \n6  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \" \"      \n7  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \"*\"      \" \"      \n8  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \"*\"      \n9  ( 1 )  \" \" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \"*\"      \n10  ( 1 ) \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \"*\"      \n11  ( 1 ) \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \"*\"  \"*\"  \"*\"    \" \"      \"*\"      \n12  ( 1 ) \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \"*\"  \"*\"  \"*\"    \"*\"      \"*\"      \n13  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \"*\"  \"*\"  \"*\"    \"*\"      \"*\"      \n14  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\"  \"*\"  \"*\" \"*\"  \"*\"  \"*\"    \"*\"      \"*\"      \n          ventSud pluieSec\n1  ( 1 )  \" \"     \" \"     \n2  ( 1 )  \" \"     \" \"     \n3  ( 1 )  \" \"     \" \"     \n4  ( 1 )  \" \"     \" \"     \n5  ( 1 )  \" \"     \"*\"     \n6  ( 1 )  \" \"     \"*\"     \n7  ( 1 )  \" \"     \"*\"     \n8  ( 1 )  \"*\"     \"*\"     \n9  ( 1 )  \"*\"     \"*\"     \n10  ( 1 ) \"*\"     \"*\"     \n11  ( 1 ) \"*\"     \"*\"     \n12  ( 1 ) \"*\"     \"*\"     \n13  ( 1 ) \"*\"     \"*\"     \n14  ( 1 ) \"*\"     \"*\"     \n\n\n\nOn obtient une table avec des étoiles qui permettent de visualiser les meilleurs modèles à \\(1,2,\\dots,8\\) variables au sens du \\(R^2\\).\n\nSélectionner le meilleur modèle au sens du \\(R^2\\). Que remarquez-vous ?\n\nplot(mod.sel,scale=\"r2\")\n\n\n\n\n\nLe meilleur modèle est le modèle complet. C’est logique puisque le \\(R^2\\) va toujours privilégier le modèle le plus complexe, c’est un critère d'ajustement.\n\nFaire de même pour le \\(C_p\\) et le \\(BIC\\). Que remarquez-vous pour les variables explicatives qualitatives ?\n\nplot(mod.sel,scale=\"bic\")\n\n\n\nplot(mod.sel,scale=\"Cp\")\n\n\n\n\n\nCes critères choisissent ici le même modèle, avec 4 variables. On remarque que les variables qualitatives ne sont pas réellement traitées comme des variables : une modalité est égale à une variable. Par conséquent, cette procédure ne permet pas vraiment de sélectionner des variables qualitatives.\n\nComparer cette méthode avec des modèles sélectionnées par la fonction step ou la fonction bestglm du package bestglm.\n\n\n\nLa fonction step permet de faire de la sélection pas à pas. Par exemple, pour une procédure descendante avec le critère \\(AIC\\) on utilisera :\nmod.step <- step(lin.complet,direction=\"backward\",trace=0)\nmod.step\nLa fonction bestglm permet quant à elle de faire des sélections exhaustive ou pas à pas, on peut l’utiliser pour tous les glm. Attention les variables qualitatives doivent être des facteurs et la variable à expliquer doit être positionnée en dernière colonne pour cette fonction.\n\nozone1 <- ozone |> mutate(vent=as.factor(vent),pluie=as.factor(pluie)) |>\n  select(-maxO3,everything())\nlibrary(bestglm)\nmodel.bglm <- bestglm(ozone1,IC=\"BIC\")\nmodel.bglm$BestModel |> summary()\n\n\nCall:\nlm(formula = y ~ ., data = data.frame(Xy[, c(bestset[-1], FALSE), \n    drop = FALSE], y = y))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.396  -8.377  -1.086   7.951  40.933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 12.63131   11.00088   1.148 0.253443    \nT12          2.76409    0.47450   5.825 6.07e-08 ***\nNe9         -2.51540    0.67585  -3.722 0.000317 ***\nVx9          1.29286    0.60218   2.147 0.034055 *  \nmaxO3v       0.35483    0.05789   6.130 1.50e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14 on 107 degrees of freedom\nMultiple R-squared:  0.7622,    Adjusted R-squared:  0.7533 \nF-statistic: 85.75 on 4 and 107 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "02-reg-comp.html#régression-sur-composantes-principales-méthodo",
    "href": "02-reg-comp.html#régression-sur-composantes-principales-méthodo",
    "title": "2  Régression sur composantes",
    "section": "2.2 Régression sur composantes principales (méthodo)",
    "text": "2.2 Régression sur composantes principales (méthodo)\nL’algorithme PCP est une méthode de réduction de dimension, elle consiste à faire un modèle linéaire MCO sur les premiers axes de l’ACP. On désigne par\n\n\\(\\mathbb X\\) la matrice qui contient les valeurs des variables explicatives que l’on suppose centrée réduite.\n\\(Z_1,\\dots,Z_p\\) les axes de l’ACP qui s’écrivent comme des combinaisons linéaires des variables explicatives : \\(Z_j=w_j^t X\\).\n\nL’algorithme PCR consiste à choisir un nombre de composantes \\(m\\) et à faire une régression MCO sur les \\(m\\) premiers axes de l’ACP : \\[Y=\\alpha_0+\\alpha_1 Z_1+\\dots+\\alpha_mZ_m+\\varepsilon.\\]\nSi on désigne par\n\n\\(x\\in\\mathbb R^d\\) une nouvelle observation que l’on a centrée réduite également;\n\\(z_1,\\dots,z_M\\) les coordonnées de \\(x\\) dans la base définie par les \\(m\\) premiers axes de l’ACP (\\(z_j=w_j^tx\\))\n\nl’algorithme PCR reverra la prévision \\[\\widehat m_{\\text{PCR}}(x)=\\widehat \\alpha_0+\\widehat \\alpha_1 z_1+\\dots+\\widehat \\alpha_mz_m.\\] Cette prévision peut s’écrire également comme une combinaison linéaire des variables explicatives (centrées réduites ou non) : \\[\\widehat m_{\\text{PCR}}(x)=\\widehat \\gamma_0+\\widehat \\gamma_1 \\tilde x_1+\\dots+\\widehat \\gamma_p \\tilde x_p=\\widehat \\beta_0+\\widehat \\beta_1 x_1+\\dots+\\widehat \\beta_p x_p,\\] \\(\\tilde x_j\\) désignant l’observation brute (non centrée réduite).\nL’exercice suivant revient sur cet algorithme et notamment sur le lien entre ces différents paramètres.\n\nExercice 2.1 (Régression PCR avec R) On considère le jeu de données Hitters dans lequel on souhaite expliquer la variable Salary par les autres variables du jeu de données. Pour simplifier le problème, on supprime les individus qui possèdent des données manquantes (il ne faut pas faire ça normalement !).\n\nlibrary(ISLR)\nHitters <- na.omit(Hitters)\n\n\nParmi les variables explicatives, certaines sont qualitatives. Expliquer comment, à l’aide de la fonction model.matrix on peut utiliser ces variables dans un modèle linéaire. On appellera X la matrice des variables explicatives construites avec cette variable.\n\nComme pour le modèle linéaire, on utilise des contraintes identifiantes. Cela revient à prendre une modalité de référence et à coder les autres modalités par 0-1.\n\n\nX <- model.matrix(Salary~.,data=Hitters)[,-1]\n\nCalculer la matrice Xcr qui correspond à la matrice X centrée réduite. On pourra utiliser la fonction scale.\n\nXcr <- scale(X)\nXbar  <- apply(X,2,mean)\nstdX <- apply(X,2,sd)\n\nA l’aide de la fonction PCA du package FactoMineR, effectuer l’ACP du tableau Xcr avec l’option scale.unit=FALSE.\n\nOn utilise ici scale.unit=FALSE car les données sont déjà centrées-réduites. Ça nous permet de contrôler cette étape.\n\n\nlibrary(FactoMineR)\nacp.hit <- PCA(Xcr,scale.unit=FALSE,graph=TRUE)\n\nRécupérer les coordonnées des individus sur les 5 premiers axes de l’ACP (variables \\(Z\\) dans le cours).\n\nCC <- acp.hit$ind$coord\n\nEffectuer la régression linéaire sur les 5 premières composantes principales et calculer les estimateurs des MCO (\\(\\widehat\\alpha_k,k=1,\\dots,5\\) dans le cours).\n\ndonnees <- cbind.data.frame(CC,Salary=Hitters$Salary)\nmod <- lm(Salary~.,data=donnees)\nalpha <- coef(mod)\nalpha\n\n(Intercept)       Dim.1       Dim.2       Dim.3       Dim.4       Dim.5 \n  535.92588   106.57139    21.64469    24.34057    37.05637   -58.52540 \n\n\n\nRemarque :\n\nOn obtient ici les estimateurs des \\(\\alpha,j=1,\\dots,5\\).\n\non peut aussi tout faire “à la main” (sans utiliser PCA)\n\n\n\nacp.main <- eigen(t(Xcr)%*%Xcr)\nU <- acp.main$vectors\nCC <- Xcr%*%(-U[,1:5])\nD <- cbind.data.frame(CC,Salary=Hitters$Salary)\nmodS <- lm(Salary~.,data=D)\ncoefS <- modS$coefficients\ncoef(modS)\n\n(Intercept)         `1`         `2`         `3`         `4`         `5` \n  535.92588   106.57139    21.64469    24.34057    37.05637    58.52540 \n\n\nEn déduire les estimateurs dans l’espace des données initiales pour les données centrées réduites, puis pour les données brutes. On pourra récupérer les vecteurs propres de l’ACP (\\(w_k\\) dans le cours) dans la sortie svd de la fonction PCA.\n\n\nPour les données centrées-réduites, les coefficients s’obtiennent avec les formules vues en cours\n\n\\[\\widehat\\beta_0=\\bar{\\mathbb Y}\\quad\\text{et}\\quad \\widehat\\beta_j=\\sum_{k=1}^m\\widehat\\alpha_kw_{kj}.\\]\n\n    W <- acp.hit$svd$V\n    V <- t(W)\n    beta0.cr <- mean(Hitters$Salary)\n    beta.cr <- as.vector(alpha[2:6])%*%V\n    beta.cr\n\n         [,1]     [,2]    [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n[1,] 28.76604 30.44702 25.8445 33.00088 33.81997 35.08779 22.35103 29.01477\n         [,9]    [,10]    [,11]    [,12]    [,13]  [,14]     [,15]    [,16]\n[1,] 29.78584 30.00201 32.06912 31.11231 31.48735 19.439 -63.20387 17.36044\n         [,17]     [,18]    [,19]\n[1,] -5.523264 -6.044002 21.74267\n\n\nPour les données brutes, on utilise les formules :\n\\[\\widehat\\gamma_0=\\widehat\\beta_0-\\sum_{j=1}^p\\widehat\\beta_j\\mu_j\\quad\\text{et}\\quad\\widehat\\gamma_j=\\frac{\\widehat\\beta_j}{\\sigma_j}.\\]\n\n    gamma0 <- beta0.cr-sum(beta.cr*Xbar/stdX)\n    gamma <- beta.cr/stdX\n    gamma0\n\n[1] -58.32022\n\n    gamma\n\n          [,1]      [,2]    [,3]     [,4]     [,5]     [,6]     [,7]       [,8]\n[1,] 0.1952793 0.6747214 2.95126 1.292134 1.306662 1.615605 4.662667 0.01268914\n           [,9]     [,10]      [,11]      [,12]    [,13]    [,14]   [,15]\n[1,] 0.04595165 0.3649987 0.09682748 0.09621344 0.119245 38.86728 -126.19\n          [,16]       [,17]      [,18]    [,19]\n[1,] 0.06201606 -0.03807032 -0.9148466 43.51629\n\n\n\n\nRetrouver les estimateurs dans l’espace des données initiales pour les données centrées réduites à l’aide de la fonction pcr du package pls.\n\nlibrary(pls)\npcr.fit <- pcr(Salary~.,data=Hitters,scale=TRUE,ncomp=19)\ncoefficients(pcr.fit,ncomp=5)\n\n, , 5 comps\n\n               Salary\nAtBat       28.766042\nHits        30.447021\nHmRun       25.844498\nRuns        33.000876\nRBI         33.819966\nWalks       35.087794\nYears       22.351033\nCAtBat      29.014768\nCHits       29.785842\nCHmRun      30.002014\nCRuns       32.069124\nCRBI        31.112315\nCWalks      31.487349\nLeagueN     19.438996\nDivisionW  -63.203872\nPutOuts     17.360440\nAssists     -5.523264\nErrors      -6.044002\nNewLeagueN  21.742668\n\n\n\nOn remarque que la fonction PCR renvoie les coefficients par rapport aux variables initiales centrées réduites. Cela fait du sens car il est dans ce cas possible de comparer les valeurs des estimateurs pour tenter d’interpréter le modèle. C’est beaucoup plus difficile à faire avec les coefficients des axes de l’ACP ou des variables intiales. Il est également important de noter que, contrairement aux estimateurs MCO du modèle linéaire Gaussien, on n’a pas d’information précise sur la loi des estimateurs, il n’est donc pas possible (ou pas facile) de faire des tests ou de calculer des intervalles de confiance.\n\nOn considère les individus suivants\n\ndf.new <- Hitters[c(1,100,80),]\n\nCalculer de 3 façons différentes les valeurs de salaire prédites par la régression sur 5 composantes principales.\n\n\nApproche classique : on utilise predict.pcr :\n\n    predict(pcr.fit,newdata=df.new,ncomp=5)\n\n, , 5 comps\n\n                Salary\n-Alan Ashby   495.0068\n-Hubie Brooks 577.9581\n-George Bell  822.0296\n\n\nOn considère les valeurs centrées réduites et on utilise : \\[\\widehat m_{\\text{PCR}}(x)=\\widehat\\beta_0+\\widehat\\beta_1x_1+\\dots+\\widehat\\beta_px_p.\\]\n\n    t(as.matrix(coefficients(pcr.fit,ncomp=5))) %*% \n      t(as.matrix(Xcr[c(1,100,80),]))+mean(Hitters$Salary)\n\n     -Alan Ashby -Hubie Brooks -George Bell\n[1,]    495.0068      577.9581     822.0296\n\n    #ou\n    beta0.cr+beta.cr%*%t(as.matrix(Xcr[c(1,100,80),]))\n\n     -Alan Ashby -Hubie Brooks -George Bell\n[1,]    495.0068      577.9581     822.0296\n\n\nOn considère les données brutes et on utilise : \\[\\widehat m_{\\text{PCR}}(x)=\\widehat\\gamma+\\widehat\\gamma_1\\tilde x_1+\\dots+\\widehat\\gamma_p\\tilde x_p.\\]\n\n    gamma0+gamma %*% t(as.matrix(X[c(1,100,80),]))\n\n     -Alan Ashby -Hubie Brooks -George Bell\n[1,]    495.0068      577.9581     822.0296\n\n\n\n\n\n\n\n\n\n\nExercice 2.2 (Composantes PCR) On rappelle que les poids \\(w_k\\) des composantes principales s’obtiennent en résolvant le problème :\n\\[\\max_{w\\in\\mathbb R^d}\\mathbf V(\\mathbb Xw)\\] \\[\\text{sous les contraintes }\\|w\\|=1,w^t\\mathbb X^t\\mathbb X w_\\ell=0, \\ell=1,\\dots,k-1.\\]\n\nMontrer \\(w_1\\) est un vecteur propre associé à la plus grande valeur propre de \\(\\mathbb X^t\\mathbb X\\).\n\nOn écrit le Lagrangien \\[L(w,\\lambda)=w^t\\mathbb X^t\\mathbb Xw-\\lambda(w^tw-1).\\] et on le dérive par rapport à \\(w\\) : \\[\\frac{\\partial L}{\\partial w}(w,\\lambda)=2\\mathbb X^t\\mathbb Xw-2\\lambda w.\\] En annulant cette dérivée, on déduit que \\(w_1\\) est un vecteur propre de \\(\\mathbb X^t\\mathbb X\\). De plus, si \\(w\\) est vecteur propre unitaire de \\(\\mathbb X^t\\mathbb X\\) associé à la valeur propre \\(\\lambda\\) on a \\(\\mathbf V(\\mathbb Xw)=\\lambda\\). On déduit que \\(w_1\\) est un vecteur propre associé à la plus grande valeur propre de \\(\\mathbb X^t\\mathbb X\\).\n\nCalculer \\(w_2\\).\n\nOn écrit le Lagrangien \\[L(w,\\lambda,\\mu)=w^t\\mathbb X^t\\mathbb Xw-\\lambda(w^tw-1)-\\mu w^t\\mathbb X^t\\mathbb Xw_1\\] et on calcule les dérivées partielles : \\[\\frac{\\partial L}{\\partial w}(w,\\lambda,\\mu)=2\\mathbb X^t\\mathbb Xw-2\\lambda w-\\mu\\mathbb X^t\\mathbb Xw_1.\\] \\[\\frac{\\partial L}{\\partial \\lambda}(w,\\lambda,\\mu)=w^tw-1\\quad\\text{et}\\quad\\frac{\\partial L}{\\partial \\mu}(w,\\lambda,\\mu)=-w^t\\mathbb X^t\\mathbb Xw_1.\\] En multipliant la première dérivée partielle par \\(w_1^t\\) et en utilisant le fait que \\(W_1\\) est un vecteur propre de \\(\\mathbb X^t\\mathbb X\\), on déduit que \\(\\mu=0\\). Par conséquent, \\(w_2\\) est un vecteur propre associé à la deuxième plus grande valeur propre de \\(\\mathbb X^t\\mathbb X\\)."
  },
  {
    "objectID": "02-reg-comp.html#régression-pls-méthodo",
    "href": "02-reg-comp.html#régression-pls-méthodo",
    "title": "2  Régression sur composantes",
    "section": "2.3 Régression PLS : méthodo",
    "text": "2.3 Régression PLS : méthodo\nLa régression PLS propose de construire également de nouvelles composantes comme des combinaisons linéaires des variables explicatives. Comme pour l’algorithme PCR, les composantes sont calculées les unes après les autres et orthogonales entre elles. La principale différence et qu’on ne cherche pas les composantes qui maximisent la variabilités des observations projetées, mais les composantes qui maximisent la colinéarité avec la cible. L’algorithme est expliqué dans l’exercice suivant.\n\nExercice 2.3 (Calcul des composantes PLS) On reprend les notations du cours : \\(\\mathbb Y\\) désigne le vecteur de la variable à expliquer et \\(\\mathbb X\\) la matrice qui contient les observations des variables explicatives. On la suppose toujours centrée réduite.\n\nOn pose \\(\\mathbb Y^{(1)}=\\mathbb Y\\) et \\(\\mathbb X^{(1)}=\\mathbb X\\). On cherche \\(Z_1=w_1^tX^{(1)}\\) qui maximise \\[\\langle \\mathbb X^{(1)}w_1,\\mathbb Y^{(1)}\\rangle\\quad\\text{sous la contrainte}\\quad\\|w\\|^2=1.\\] Cela revient à cherche la combinaison linéaire des colonnes de \\(\\mathbb X^{(1)}\\) la plus corrélée à \\(\\mathbb Y^{(1)}\\). Calculer cette première composante.\n\nOn écrit le lagrangien \\[L(x,\\lambda)={\\mathbb Y^{(1)}}^t\\mathbb X^{(1)}w_1-\\frac{1}{2}\\lambda(\\|w_1\\|^2-1)\\] En dérivant par rapport à \\(w\\) et \\(\\lambda\\) on obtient les équations \\[\\left\\{\n\\begin{array}{l}\n{\\mathbb X^{(1)}}^t\\mathbb Y^{(1)}-\\lambda w_1=0 \\\\\n\\|w_1\\|^2=1\n\\end{array}\\right.\\] La solution est donnée par \\[w_1=\\frac{{\\mathbb X^{(1)}}^t\\mathbb Y^{(1)}}{{\\|\\mathbb X^{(1)}}^t\\mathbb Y^{(1)}\\|}.\\]\n\nOn pose \\(Z_1=w_1^tX^{(1)}\\) et \\(\\mathbb Z_1=\\mathbb X^{(1)}w_1\\). On considère le modèle de régression linéaire \\[Y^{(1)}=\\alpha_0+\\alpha_1Z_1+\\varepsilon.\\] Exprimer les estimateurs MCO de \\(\\alpha=(\\alpha_0,\\alpha_1)\\) en fonction de \\(\\mathbb Z^{(1)}\\) et \\(\\mathbb Y^{(1)}\\).\n\nOn déduit \\[\\widehat \\alpha_0=\\bar{\\mathbb Y}^{(1)}-\\widehat \\alpha_1\\bar{\\mathbb Z}_1=\\bar{\\mathbb Y}^{(1)}\\] car \\(\\bar{\\mathbb Z}_1=0\\) puisque \\(\\mathbb X^{(1)}\\) est centrée. Le second estimateur s’obtient par \\[\\widehat \\alpha_1=\\frac{\\langle \\mathbb Z_1,\\mathbb Y^{(1)}\\rangle}{\\langle \\mathbb Z_1,\\mathbb Z_1\\rangle}.\\]\n\nOn passe maintenant à la deuxième composante. On cherche à expliquer la partie résiduelle \\[\\mathbb Y^{(2)}=P_{Z_1^\\perp}(\\mathbb Y^{(1)})=\\widehat\\varepsilon_1=\\mathbb Y^{(1)}-\\widehat{\\mathbb Y}^{(1)}\\] par la “meilleure” combinaison linéaire orthogonale à \\(Z_1\\). On orthogonalise chaque \\(\\tilde{\\mathbb X}_j^{(1)}\\) par rapport à \\(\\mathbb Z_1\\) : \\[{\\mathbb X}_j^{(2)}=P_{\\mathbb Z_1^\\perp}({\\mathbb X}_j^{(1)})=(\\text{Id}-P_{\\mathbb Z_1})({\\mathbb X}_j^{(1)})={\\mathbb X}_j^{(1)}-\\frac{\\langle \\mathbb Z_1,{\\mathbb X}_j^{(1)}\\rangle}{\\langle \\mathbb Z_1,\\mathbb Z_1\\rangle}\\mathbb Z_1.\\] et on déduit \\(w_2\\) comme \\(w_1\\) : \\(w_2=\\tilde{\\mathbb X}^{(2)'}\\mathbb Y^{(2)}\\). On considère ensuite le modèle \\(Y^{(2)}=\\alpha_2Z_2+\\varepsilon\\). Exprimer l’estimateur des MCO de \\(\\alpha_2\\) en fonction de \\(\\mathbb Z_2=\\mathbb X^{(2)}w_2\\) et \\(\\mathbb Y\\).\n\nOn a \\[\\widehat\\alpha_2=\\frac{\\langle \\mathbb Z_2,\\mathbb Y^{(2)}\\rangle}{\\langle \\mathbb Z_2,\\mathbb Z_2\\rangle}=\\frac{\\langle \\mathbb Z_2,\\mathbb Y-\\widehat{\\mathbb Y}^{(1)}\\rangle}{\\langle \\mathbb Z_2,\\mathbb Z_2\\rangle}=\\frac{\\langle \\mathbb Z_2,\\mathbb Y\\rangle}{\\langle \\mathbb Z_2,\\mathbb Z_2\\rangle}\\] car \\(\\widehat{\\mathbb Y}^{(1)}=\\widehat \\alpha_0+\\widehat \\alpha_1\\mathbb Z_1\\) est orthogonal à \\(\\mathbb Z_2\\).\n\n\n\n\nExercice 2.4 (Régression PLS sur R) On considère les mêmes données que précédemment.\n\nA l’aide du vecteur \\(\\mathbb Y\\) (Salary) et de la matrice des \\(\\mathbb X\\) centrées réduites calculées dans l’Exercice 2.1, calculer la première composante PLS \\(\\mathbb Z_1\\).\n\nY <- as.vector(Hitters$Salary)\nw1 <- t(Xcr)%*%Y\nw1\n\n                  [,1]\nAtBat       46659.1995\nHits        51848.3247\nHmRun       40543.5500\nRuns        49624.3823\nRBI         53122.7240\nWalks       52462.0450\nYears       47354.8899\nCAtBat      62185.5603\nCHits       64877.3193\nCHmRun      62043.1671\nCRuns       66504.6198\nCRBI        67011.4288\nCWalks      57893.5821\nLeagueN     -1688.0134\nDivisionW  -22753.8726\nPutOuts     35514.7030\nAssists      3006.3756\nErrors       -638.3256\nNewLeagueN   -335.0136\n\nZ1 <- Xcr%*%w1\n\nEn déduire le coefficient associé à cette première composante en considérant le modèle \\[Y=\\alpha_1 Z_1+\\varepsilon.\\]\n\ndf <- data.frame(Z1,Y)\nmod1 <- lm(Y~Z1-1,data=df)\nalpha1 <- coef(mod1)\nalpha1\n\n          Z1 \n0.0005367014 \n\n\nEn déduire les coefficients en fonction des variables initiales (centrées réduites) de la régression PLS à une composante \\[Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p+\\varepsilon.\\]\n\nalpha1*w1\n\n                  [,1]\nAtBat       25.0420570\nHits        27.8270677\nHmRun       21.7597795\nRuns        26.6334747\nRBI         28.5110396\nWalks       28.1564522\nYears       25.4154350\nCAtBat      33.3750764\nCHits       34.8197471\nCHmRun      33.2986538\nCRuns       35.6931216\nCRBI        35.9651267\nCWalks      31.0715657\nLeagueN     -0.9059591\nDivisionW  -12.2120349\nPutOuts     19.0607903\nAssists      1.6135259\nErrors      -0.3425902\nNewLeagueN  -0.1798022\n\n\nRetrouver ces coefficients en utilisant la fonction plsr.\n\npls.fit <- plsr(Salary~.,data=Hitters,scale=TRUE)\ncoefficients(pls.fit,ncomp = 1)\n\n, , 1 comps\n\n                Salary\nAtBat       25.0420570\nHits        27.8270677\nHmRun       21.7597795\nRuns        26.6334747\nRBI         28.5110396\nWalks       28.1564522\nYears       25.4154350\nCAtBat      33.3750764\nCHits       34.8197471\nCHmRun      33.2986538\nCRuns       35.6931216\nCRBI        35.9651267\nCWalks      31.0715657\nLeagueN     -0.9059591\nDivisionW  -12.2120349\nPutOuts     19.0607903\nAssists      1.6135259\nErrors      -0.3425902\nNewLeagueN  -0.1798022"
  },
  {
    "objectID": "02-reg-comp.html#comparaison-pcr-vs-pls.",
    "href": "02-reg-comp.html#comparaison-pcr-vs-pls.",
    "title": "2  Régression sur composantes",
    "section": "2.4 Comparaison : PCR vs PLS.",
    "text": "2.4 Comparaison : PCR vs PLS.\n\nSéparer le jeu de données (Hitters toujours) en un échantillon d’apprentissage de taille 200 et un échantillon test de taille 63.\n\nset.seed(1234)\nperm <- sample(nrow(Hitters))\ndapp <- Hitters[perm[1:200],]\ndtest <- Hitters[perm[201:nrow(Hitters)],]\n\nAvec les données d’apprentissage uniquement construire les régressions PCR et PLS. On choisira les nombres de composantes par validation croisée.\n\nchoix.pcr <- pcr(Salary~.,data=dapp,validation=\"CV\")\nncomp.pcr <- which.min(choix.pcr$validation$PRESS)\nncomp.pcr\n\n[1] 4\n\n\n\nchoix.pls <- plsr(Salary~.,data=dapp,validation=\"CV\")\nncomp.pls <- which.min(choix.pls$validation$PRESS)\nncomp.pls\n\n[1] 3\n\n\nComparer les deux méthodes en utilisant l’échantillon de validation. On pourra également utiliser un modèle linéaire classique.\n\nmod.lin <- lm(Salary~.,data=dapp)\n\n\nprev <- data.frame(\n  lin=predict(mod.lin,newdata=dtest),\n  pcr=as.vector(predict(choix.pcr,newdata = dtest,ncomp=ncomp.pcr)),\n  pls=as.vector(predict(choix.pls,newdata = dtest,ncomp=ncomp.pls)),\n  obs=dtest$Salary\n)\n\n\nprev |> summarize_at(1:3,~(mean((.-obs)^2))) |> sqrt()\n\n       lin      pcr      pls\n1 334.8819 348.3943 342.7771\n\n\nComparer ces méthodes à l’aide d’une validation croisée 10 blocs.\n\nAttention il ne s’agit pas ici de sélectionner les nombres de composantes par validation croisée. On veut comparer :\n\nl’algorithme PCR qui sélectionne le nombre de composantes par validation croisée à\nl’algorithme PLS qui sélectionne le nombre de composantes par validation croisée.\n\nOn définit d’abord les 10 blocs pour la validation croisée :\n\n\nset.seed(1234)\nbloc <- sample(1:10,nrow(Hitters),replace=TRUE)\ntable(bloc)\n\nbloc\n 1  2  3  4  5  6  7  8  9 10 \n19 22 31 29 28 39 19 26 25 25 \n\n\n\nPuis on fait la validation croisée (en sélectionnant le nombre de composantes par validation croisée) à chaque étape :\n\n\nset.seed(4321)\nprev <- data.frame(matrix(0,nrow=nrow(Hitters),ncol=3))\nnames(prev) <- c(\"lin\",\"PCR\",\"PLS\")\nfor (k in 1:10){\n#  print(k)\n  ind.test <- bloc==k\n  dapp <- Hitters[!ind.test,]\n  dtest <- Hitters[ind.test,]\n  choix.pcr <- pcr(Salary~.,data=dapp,validation=\"CV\")\n  ncomp.pcr <- which.min(choix.pcr$validation$PRESS)\n  choix.pls <- plsr(Salary~.,data=dapp,validation=\"CV\")\n  ncomp.pls <- which.min(choix.pls$validation$PRESS)\n  mod.lin <- lm(Salary~.,data=dapp)\n  prev[ind.test,] <- data.frame(\n    lin=predict(mod.lin,newdata=dtest),\n    PCR=as.vector(predict(choix.pcr,newdata = dtest,ncomp=ncomp.pcr)),\n    PLS=as.vector(predict(choix.pls,newdata = dtest,ncomp=ncomp.pls)))\n}\n\n\nprev |> mutate(obs=Hitters$Salary) |> \n  summarize_at(1:3,~(mean((.-obs)^2))) |> sqrt()\n\n       lin      PCR      PLS\n1 340.0631 343.8019 350.6712\n\n\n\nOn compare à un modèle qui prédit toujours la moyenne :\n\n\nvar(Hitters$Salary) |> sqrt()\n\n[1] 451.1187\n\n\n\nOn peut retenter l’analyse en considérant toutes les interactions d’ordre 2 :\n\n\nset.seed(54321)\nprev1 <- data.frame(matrix(0,nrow=nrow(Hitters),ncol=3))\nnames(prev1) <- c(\"lin\",\"PCR\",\"PLS\")\nfor (k in 1:10){\n#  print(k)\n  ind.test <- bloc==k\n  dapp <- Hitters[!ind.test,]\n  dtest <- Hitters[ind.test,]\n  choix.pcr <- pcr(Salary~.^2,data=dapp,validation=\"CV\")\n  ncomp.pcr <- which.min(choix.pcr$validation$PRESS)\n  choix.pls <- plsr(Salary~.^2,data=dapp,validation=\"CV\")\n  ncomp.pls <- which.min(choix.pls$validation$PRESS)\n  mod.lin <- lm(Salary~.^2,data=dapp)\n  prev1[ind.test,] <- data.frame(\n    lin=predict(mod.lin,newdata=dtest),\n    PCR=as.vector(predict(choix.pcr,newdata = dtest,ncomp=ncomp.pcr)),\n    PLS=as.vector(predict(choix.pls,newdata = dtest,ncomp=ncomp.pls))\n  )\n}\n\n\nOn obtient les performances suivantes :\n\n\nprev1 |> mutate(obs=Hitters$Salary) |> \n  summarize_at(1:3,~(mean((.-obs)^2))) |> sqrt()\n\n       lin      PCR      PLS\n1 1494.847 330.0474 349.1116\n\n\n\nOn mesure bien l’intérêt de réduire la dimension dans ce nouveau contexte."
  }
]