```{r,child='_preamble.qmd'}
```

# Régression sur composantes {#reg-comp}


Les performances des estimateurs classiques (MCO) des paramètres du modèle linéaire

$$Y=\beta_0+\beta_1X_1+\dots+\beta_dX_d+\varepsilon$$ peuvent se dégrader lorsque la dimension $d$ est grande ou en présence de dépendance linéaire entre les variables explicatives. Les régressions sur composantes consistent à trouver de nouvelles composantes $Z_k,j=k,\dots,q$ avec $q\leq p$ qui s'écrivent le plus souvent comme des combinaisons linéaires des $X_j$ dans l'idée de diminuer le nombre de paramètres du modèle ou la dépendance entre les covariables. Il existe plusieurs façons de construire ces composantes, dans cette partie nous proposons :

-   la **régression sous composantes principales (PCR)** : il s'agit de faire simplement une ACP sur la matrice des variables explicatives ;
-   la **régression partial least square (PLS)** qui fait intervenir la variable cible dans la construction des composantes.

Nous commençons par un bref rappel sur la sélection de variables.

## Sélection de variables

On considère le jeu de données `ozone.txt` où on cherche à expliquer la concentration maximale en ozone relevée sur une journée (variable `maxO3`) par d'autres variables essentiellement météorologiques.

```{r}
ozone <- read.table("data/ozone.txt")
head(ozone)
```

1.  Ajuster un modèle linéaire avec `lm` et analyser la pertinence des variables explicatives dans le modèle.

    ```{r,teacher=correct}
    lin.complet <- lm(maxO3~.,data=ozone)
    summary(lin.complet)
    anova(lin.complet)
    ```

    ::: {.corR data-latex=""}
    Il semble que quelques variables ne sont pas nécessaires dans le modèle.
    :::

2.  Expliquer les sorties de la commande

    ```{r}
    library(leaps)
    mod.sel <- regsubsets(maxO3~.,data=ozone,nvmax=14)
    summary(mod.sel)
    ```

    ::: {.corR data-latex=""}
    On obtient une table avec des étoiles qui permettent de visualiser les meilleurs modèles à $1,2,\dots,8$ variables au sens du $R^2$.
    :::

3.  Sélectionner le meilleur modèle au sens du $R^2$. Que remarquez-vous ?

    ```{r,teacher=correct}
    plot(mod.sel,scale="r2")
    ```

    ::: {.corR data-latex=""}
    Le meilleur modèle est le modèle complet. C'est logique puisque le $R^2$ va toujours privilégier le modèle le plus complexe, c'est un critère `d'ajustement`.
    :::

4.  Faire de même pour le $C_p$ et le $BIC$. Que remarquez-vous pour les variables explicatives qualitatives ?

    ```{r,teacher=correct}
    plot(mod.sel,scale="bic")
    plot(mod.sel,scale="Cp")
    ```

    ::: {.corR data-latex=""}
    Ces critères choisissent ici le même modèle, avec 4 variables. On remarque que les variables qualitatives ne sont `pas réellement traitées comme des variables` : une modalité est égale à une variable. Par conséquent, cette procédure ne permet pas vraiment de sélectionner des variables qualitatives.
    :::

5.  Comparer cette méthode avec des modèles sélectionnées par la fonction `step` ou la fonction `bestglm` du package `bestglm`.\

    ::: {.corR data-latex=""}
    -   La fonction `step` permet de faire de la sélection **pas à pas**. Par exemple, pour une procédure `descendante` avec le critère $AIC$ on utilisera :

        ``` {rteacher="correct"}
        mod.step <- step(lin.complet,direction="backward",trace=0)
        mod.step
        ```

        La fonction `bestglm` permet quant à elle de faire des sélections exhaustive ou pas à pas, on peut l'utiliser pour tous les **glm**. Attention les variables qualitatives doivent être des facteurs et la variable à expliquer doit être positionnée en dernière colonne pour cette fonction.

        ```{r chunk-bestglm,teacher=correct,cache=TRUE}
        ozone1 <- ozone |> mutate(vent=as.factor(vent),pluie=as.factor(pluie)) |>
          select(-maxO3,everything())
        library(bestglm)
        model.bglm <- bestglm(ozone1,IC="BIC")
        model.bglm$BestModel |> summary()
        ```
    :::

## Régression sur composantes principales (méthodo)

L'algorithme **PCP** est une méthode de réduction de dimension, elle consiste à faire un modèle linéaire **MCO** sur les premiers axes de l'**ACP**. On désigne par

-   $\mathbb X$ la matrice qui contient les valeurs des variables explicatives que l'on suppose centrée réduite.
-   $Z_1,\dots,Z_p$ les axes de l'ACP qui s'écrivent comme des combinaisons linéaires des variables explicatives : $Z_j=w_j^t X$.

L'algorithme **PCR** consiste à choisir un nombre de composantes $m$ et à faire une régression MCO sur les $m$ premiers axes de l'ACP : $$Y=\alpha_0+\alpha_1 Z_1+\dots+\alpha_mZ_m+\varepsilon.$$

Si on désigne par

-   $x\in\R^d$ une nouvelle observation que l'on a centrée réduite également;
-   $z_1,\dots,z_M$ les coordonnées de $x$ dans la base définie par les $m$ premiers axes de l'ACP ($z_j=w_j^tx$)

l'algorithme **PCR** reverra la prévision $$\widehat m_{\text{PCR}}(x)=\widehat \alpha_0+\widehat \alpha_1 z_1+\dots+\widehat \alpha_mz_m.$$ Cette prévision peut s'écrire également comme une combinaison linéaire des variables explicatives (centrées réduites ou non) : $$\widehat m_{\text{PCR}}(x)=\widehat \gamma_0+\widehat \gamma_1 \tilde x_1+\dots+\widehat \gamma_p \tilde x_p=\widehat \beta_0+\widehat \beta_1 x_1+\dots+\widehat \beta_p x_p,$$ $\tilde x_j$ désignant l'observation brute (non centrée réduite).

L'exercice suivant revient sur cet algorithme et notamment sur le lien entre ces différents paramètres.

::: {#exr-exo-methodo-pcr name="Régression PCR avec R"}
On considère le jeu de données **Hitters** dans lequel on souhaite expliquer la variable **Salary** par les autres variables du jeu de données. Pour simplifier le problème, on supprime les individus qui possèdent des données manquantes (il ne faut pas faire ça normalement !).

```{r}
library(ISLR)
Hitters <- na.omit(Hitters)
```

1.  Parmi les variables explicatives, certaines sont qualitatives. Expliquer comment, à l'aide de la fonction **model.matrix** on peut utiliser ces variables dans un modèle linéaire. On appellera **X** la matrice des variables explicatives construites avec cette variable.

    ::: {.corR data-latex=""}
    Comme pour le modèle linéaire, on utilise des contraintes identifiantes. Cela revient à prendre une modalité de référence et à coder les autres modalités par 0-1.
    :::

    ```{r,teacher=correct}
    X <- model.matrix(Salary~.,data=Hitters)[,-1]
    ```

2.  Calculer la matrice **Xcr** qui correspond à la matrice **X** centrée réduite. On pourra utiliser la fonction `scale`.

    ```{r,teacher=correct}
    Xcr <- scale(X)
    Xbar  <- apply(X,2,mean)
    stdX <- apply(X,2,sd)
    ```

3.  A l'aide de la fonction `PCA` du package **FactoMineR**, effectuer l'ACP du tableau **Xcr** avec l'option `scale.unit=FALSE`.

    ::: {.corR data-latex=""}
    On utilise ici `scale.unit=FALSE` car les données sont déjà centrées-réduites. Ça nous permet de contrôler cette étape.
    :::

    ```{r,teacher=correct}
    library(FactoMineR)
    acp.hit <- PCA(Xcr,scale.unit=FALSE,graph=TRUE)
    ```

4.  Récupérer les coordonnées des individus sur les 5 premiers axes de l'ACP (variables $Z$ dans le cours).

    ```{r,teacher=correct}
    CC <- acp.hit$ind$coord
    ```

5.  Effectuer la régression linéaire sur les 5 premières composantes principales et calculer les estimateurs des MCO ($\widehat\alpha_k,k=1,\dots,5$ dans le cours).

    ```{r,teacher=correct}
    donnees <- cbind.data.frame(CC,Salary=Hitters$Salary)
    mod <- lm(Salary~.,data=donnees)
    alpha <- coef(mod)
    alpha
    ```

    ::: {.corR data-latex=""}
    **Remarque** :

    -   On obtient ici les estimateurs des $\alpha,j=1,\dots,5$.\
    -   on peut aussi tout faire "à la main" (sans utiliser **PCA**)
    :::

    ```{r,teacher=correct}
    acp.main <- eigen(t(Xcr)%*%Xcr)
    U <- acp.main$vectors
    CC <- Xcr%*%(-U[,1:5])
    D <- cbind.data.frame(CC,Salary=Hitters$Salary)
    modS <- lm(Salary~.,data=D)
    coefS <- modS$coefficients
    coef(modS)
    ```

6.  En déduire les estimateurs dans l'espace des données initiales pour les données centrées réduites, puis pour les données brutes. On pourra récupérer les vecteurs propres de l'ACP ($w_k$ dans le cours) dans la sortie **svd** de la fonction **PCA**.

    ::: {.corR data-latex=""}
    -   Pour les données centrées-réduites, les coefficients s'obtiennent avec les formules vues en cours\

        $$\widehat\beta_0=\bar{\mathbb Y}\quad\text{et}\quad \widehat\beta_j=\sum_{k=1}^m\widehat\alpha_kw_{kj}.$$

        ```{r,teacher=correct}
            W <- acp.hit$svd$V
            V <- t(W)
            beta0.cr <- mean(Hitters$Salary)
            beta.cr <- as.vector(alpha[2:6])%*%V
            beta.cr
        ```

    -   Pour les données brutes, on utilise les formules :

        $$\widehat\gamma_0=\widehat\beta_0-\sum_{j=1}^p\widehat\beta_j\mu_j\quad\text{et}\quad\widehat\gamma_j=\frac{\widehat\beta_j}{\sigma_j}.$$

        ```{r,teacher=correct}
            gamma0 <- beta0.cr-sum(beta.cr*Xbar/stdX)
            gamma <- beta.cr/stdX
            gamma0
            gamma
        ```
    :::

7.  Retrouver les estimateurs dans l'espace des données initiales pour les données centrées réduites à l'aide de la fonction `pcr` du package **pls**.

    ```{r,teacher=correct}
    library(pls)
    pcr.fit <- pcr(Salary~.,data=Hitters,scale=TRUE,ncomp=19)
    coefficients(pcr.fit,ncomp=5)
    ```

    ::: {.corR data-latex=""}
    On remarque que la fonction **PCR** renvoie les coefficients par rapport aux variables initiales centrées réduites. Cela fait du sens car il est dans ce cas possible de comparer les valeurs des estimateurs pour tenter d'interpréter le modèle. C'est beaucoup plus difficile à faire avec les coefficients des axes de l'ACP ou des variables intiales. Il est également important de noter que, contrairement aux estimateurs MCO du modèle linéaire Gaussien, on n'a pas d'information précise sur la loi des estimateurs, il n'est donc pas possible (ou pas facile) de faire des tests ou de calculer des intervalles de confiance.
    :::

8.  On considère les individus suivants

    ```{r}
    df.new <- Hitters[c(1,100,80),]
    ```

    Calculer de 3 façons différentes les valeurs de salaire prédites par la régression sur 5 composantes principales.

    ::: {.corR data-latex=""}
    -   Approche classique : on utilise `predict.pcr` :

        ```{r,teacher=correct}
            predict(pcr.fit,newdata=df.new,ncomp=5)
        ```

    -   On considère les valeurs centrées réduites et on utilise : $$\widehat m_{\text{PCR}}(x)=\widehat\beta_0+\widehat\beta_1x_1+\dots+\widehat\beta_px_p.$$

        ```{r,teacher=correct}
            t(as.matrix(coefficients(pcr.fit,ncomp=5))) %*% 
              t(as.matrix(Xcr[c(1,100,80),]))+mean(Hitters$Salary)
            #ou
            beta0.cr+beta.cr%*%t(as.matrix(Xcr[c(1,100,80),]))
        ```

    -   On considère les données brutes et on utilise : $$\widehat m_{\text{PCR}}(x)=\widehat\gamma+\widehat\gamma_1\tilde x_1+\dots+\widehat\gamma_p\tilde x_p.$$

        ```{r,teacher=correct}
            gamma0+gamma %*% t(as.matrix(X[c(1,100,80),]))
        ```
    :::

```{r echo=FALSE,eval=FALSE}
mod.lin <- lm(Salary~.,data=Hitters)
predict(mod.lin,newdata = df.new)
predict(pcr.fit,newdata=df.new,ncomp=19)

```
:::

::: {#exr-exo-cal-compPCR name="Composantes PCR"}
On rappelle que les poids $w_k$ des composantes principales s'obtiennent en résolvant le problème :

$$\max_{w\in\R^d}\var(\mathbb Xw)$$ $$\text{sous les contraintes }\|w\|=1,w^t\mathbb X^t\mathbb X w_\ell=0, \ell=1,\dots,k-1.$$

1.  Montrer $w_1$ est un vecteur propre associé à la plus grande valeur propre de $\mathbb X^t\mathbb X$.

    ::: {.correction data-latex=""}
    On écrit le Lagrangien $$L(w,\lambda)=w^t\mathbb X^t\mathbb Xw-\lambda(w^tw-1).$$ et on le dérive par rapport à $w$ : $$\frac{\partial L}{\partial w}(w,\lambda)=2\mathbb X^t\mathbb Xw-2\lambda w.$$ En annulant cette dérivée, on déduit que $w_1$ est un vecteur propre de $\mathbb X^t\mathbb X$. De plus, si $w$ est vecteur propre unitaire de $\mathbb X^t\mathbb X$ associé à la valeur propre $\lambda$ on a $\var(\mathbb Xw)=\lambda$. On déduit que $w_1$ est un vecteur propre associé à la plus grande valeur propre de $\mathbb X^t\mathbb X$.
    :::

2.  Calculer $w_2$.

    ::: {.correction data-latex=""}
    On écrit le Lagrangien $$L(w,\lambda,\mu)=w^t\mathbb X^t\mathbb Xw-\lambda(w^tw-1)-\mu w^t\mathbb X^t\mathbb Xw_1$$ et on calcule les dérivées partielles : $$\frac{\partial L}{\partial w}(w,\lambda,\mu)=2\mathbb X^t\mathbb Xw-2\lambda w-\mu\mathbb X^t\mathbb Xw_1.$$ $$\frac{\partial L}{\partial \lambda}(w,\lambda,\mu)=w^tw-1\quad\text{et}\quad\frac{\partial L}{\partial \mu}(w,\lambda,\mu)=-w^t\mathbb X^t\mathbb Xw_1.$$ En multipliant la première dérivée partielle par $w_1^t$ et en utilisant le fait que $W_1$ est un vecteur propre de $\mathbb X^t\mathbb X$, on déduit que $\mu=0$. Par conséquent, $w_2$ est un vecteur propre associé à la deuxième plus grande valeur propre de $\mathbb X^t\mathbb X$.
    :::
:::

## Régression PLS : méthodo

La régression **PLS** propose de construire également de nouvelles composantes comme des combinaisons linéaires des variables explicatives. Comme pour l'algorithme **PCR**, les composantes sont calculées les unes après les autres et orthogonales entre elles. La principale différence et qu'on ne cherche pas les composantes qui maximisent la variabilités des observations projetées, mais les composantes qui maximisent la colinéarité avec la cible. L'algorithme est expliqué dans l'exercice suivant.

::: {#exr-exo-calc-comp-pls name="Calcul des composantes PLS"}
On reprend les notations du cours : $\mathbb Y$ désigne le vecteur de la variable à expliquer et $\mathbb X$ la matrice qui contient les observations des variables explicatives. On la suppose toujours centrée réduite.

1.  On pose $\mathbb Y^{(1)}=\mathbb Y$ et $\mathbb X^{(1)}=\mathbb X$. On cherche $Z_1=w_1^tX^{(1)}$ qui maximise $$\langle \mathbb X^{(1)}w_1,\mathbb Y^{(1)}\rangle\quad\text{sous la contrainte}\quad\|w\|^2=1.$$ Cela revient à cherche la combinaison linéaire des colonnes de $\mathbb X^{(1)}$ la plus corrélée à $\mathbb Y^{(1)}$. Calculer cette première composante.

    ::: {.correction data-latex=""}
    On écrit le lagrangien $$L(x,\lambda)={\mathbb Y^{(1)}}^t\mathbb X^{(1)}w_1-\frac{1}{2}\lambda(\|w_1\|^2-1)$$ En dérivant par rapport à $w$ et $\lambda$ on obtient les équations $$\left\{
    \begin{array}{l}
    {\mathbb X^{(1)}}^t\mathbb Y^{(1)}-\lambda w_1=0 \\
    \|w_1\|^2=1
    \end{array}\right.$$ La solution est donnée par $$w_1=\frac{{\mathbb X^{(1)}}^t\mathbb Y^{(1)}}{{\|\mathbb X^{(1)}}^t\mathbb Y^{(1)}\|}.$$
    :::

2.  On pose $Z_1=w_1^tX^{(1)}$ et $\mathbb Z_1=\mathbb X^{(1)}w_1$. On considère le modèle de régression linéaire $$Y^{(1)}=\alpha_0+\alpha_1Z_1+\varepsilon.$$ Exprimer les estimateurs MCO de $\alpha=(\alpha_0,\alpha_1)$ en fonction de $\mathbb Z^{(1)}$ et $\mathbb Y^{(1)}$.

    ::: {.correction data-latex=""}
    On déduit $$\widehat \alpha_0=\bar{\mathbb Y}^{(1)}-\widehat \alpha_1\bar{\mathbb Z}_1=\bar{\mathbb Y}^{(1)}$$ car $\bar{\mathbb Z}_1=0$ puisque $\mathbb X^{(1)}$ est centrée. Le second estimateur s'obtient par $$\widehat \alpha_1=\frac{\ps{\mathbb Z_1}{\mathbb Y^{(1)}}}{\ps{\mathbb Z_1}{\mathbb Z_1}}.$$
    :::

3.  On passe maintenant à la deuxième composante. On cherche à expliquer la partie résiduelle $$\mathbb Y^{(2)}=P_{Z_1^\perp}(\mathbb Y^{(1)})=\widehat\varepsilon_1=\mathbb Y^{(1)}-\widehat{\mathbb Y}^{(1)}$$ par la "meilleure" combinaison linéaire orthogonale à $Z_1$. On orthogonalise chaque $\tilde{\mathbb X}_j^{(1)}$ par rapport à $\mathbb Z_1$ : $${\mathbb X}_j^{(2)}=P_{\mathbb Z_1^\perp}({\mathbb X}_j^{(1)})=(\text{Id}-P_{\mathbb Z_1})({\mathbb X}_j^{(1)})={\mathbb X}_j^{(1)}-\frac{\langle \mathbb Z_1,{\mathbb X}_j^{(1)}\rangle}{\langle \mathbb Z_1,\mathbb Z_1\rangle}\mathbb Z_1.$$ et on déduit $w_2$ comme $w_1$ : $w_2=\tilde{\mathbb X}^{(2)'}\mathbb Y^{(2)}$. On considère ensuite le modèle $Y^{(2)}=\alpha_2Z_2+\varepsilon$. Exprimer l'estimateur des MCO de $\alpha_2$ en fonction de $\mathbb Z_2=\mathbb X^{(2)}w_2$ et $\mathbb Y$.

    ::: {.correction data-latex=""}
    On a $$\widehat\alpha_2=\frac{\ps{\mathbb Z_2}{\mathbb Y^{(2)}}}{\ps{\mathbb Z_2}{\mathbb Z_2}}=\frac{\ps{\mathbb Z_2}{\mathbb Y-\widehat{\mathbb Y}^{(1)}}}{\ps{\mathbb Z_2}{\mathbb Z_2}}=\frac{\ps{\mathbb Z_2}{\mathbb Y}}{\ps{\mathbb Z_2}{\mathbb Z_2}}$$ car $\widehat{\mathbb Y}^{(1)}=\widehat \alpha_0+\widehat \alpha_1\mathbb Z_1$ est orthogonal à $\mathbb Z_2$.
    :::
:::

::: {#exr-exo-methodo-pls name="Régression PLS sur R"}
On considère les mêmes données que précédemment.

1.  A l'aide du vecteur $\mathbb Y$ (*Salary*) et de la matrice des $\mathbb X$ centrées réduites calculées dans l'@exr-exo-methodo-pcr, calculer la première composante **PLS** $\mathbb Z_1$.

    ```{r,teacher=correct}
    Y <- as.vector(Hitters$Salary)
    w1 <- t(Xcr)%*%Y
    w1
    Z1 <- Xcr%*%w1
    ```

2.  En déduire le coefficient associé à cette première composante en considérant le modèle $$Y=\alpha_1 Z_1+\varepsilon.$$

    ```{r,teacher=correct}
    df <- data.frame(Z1,Y)
    mod1 <- lm(Y~Z1-1,data=df)
    alpha1 <- coef(mod1)
    alpha1
    ```

3.  En déduire les coefficients en fonction des variables initiales (centrées réduites) de la régression PLS à une composante $$Y=\beta_0+\beta_1X_1+\dots+\beta_pX_p+\varepsilon.$$

    ```{r,teacher=correct}
    alpha1*w1
    ```

4.  Retrouver ces coefficients en utilisant la fonction `plsr`.

    ```{r,teacher=correct}
    pls.fit <- plsr(Salary~.,data=Hitters,scale=TRUE)
    coefficients(pls.fit,ncomp = 1)
    ```
:::

## Comparaison : PCR vs PLS.

1.  Séparer le jeu de données (`Hitters` toujours) en un échantillon d'apprentissage de taille 200 et un échantillon test de taille 63.

    ```{r,teacher=correct}
    set.seed(1234)
    perm <- sample(nrow(Hitters))
    dapp <- Hitters[perm[1:200],]
    dtest <- Hitters[perm[201:nrow(Hitters)],]
    ```

2.  Avec les données d'apprentissage uniquement construire les régressions PCR et PLS. On choisira les nombres de composantes par validation croisée.

    ```{r,teacher=correct}
    choix.pcr <- pcr(Salary~.,data=dapp,validation="CV")
    ncomp.pcr <- which.min(choix.pcr$validation$PRESS)
    ncomp.pcr
    ```

    ```{r,teacher=correct}
    choix.pls <- plsr(Salary~.,data=dapp,validation="CV")
    ncomp.pls <- which.min(choix.pls$validation$PRESS)
    ncomp.pls
    ```

3.  Comparer les deux méthodes en utilisant l'échantillon de validation. On pourra également utiliser un modèle linéaire classique.

    ```{r,teacher=correct}
    mod.lin <- lm(Salary~.,data=dapp)
    ```

    ```{r,teacher=correct}
    prev <- data.frame(
      lin=predict(mod.lin,newdata=dtest),
      pcr=as.vector(predict(choix.pcr,newdata = dtest,ncomp=ncomp.pcr)),
      pls=as.vector(predict(choix.pls,newdata = dtest,ncomp=ncomp.pls)),
      obs=dtest$Salary
    )
    ```

    ```{r,teacher=correct}
    prev |> summarize_at(1:3,~(mean((.-obs)^2))) |> sqrt()
    ```

4.  Comparer ces méthodes à l'aide d'une validation croisée 10 blocs.

    ::: {.corR data-latex=""}
    **Attention** il ne s'agit pas ici de sélectionner les nombres de composantes par validation croisée. On veut comparer :

    -   l'algorithme **PCR** qui sélectionne le nombre de composantes par validation croisée à

    -   l'algorithme **PLS** qui sélectionne le nombre de composantes par validation croisée.

    On définit d'abord les 10 blocs pour la validation croisée :
    :::

    ```{r,teacher=correct}
    set.seed(1234)
    bloc <- sample(1:10,nrow(Hitters),replace=TRUE)
    table(bloc)
    ```

    ::: {.corR data-latex=""}
    Puis on fait la validation croisée (en sélectionnant le nombre de composantes par validation croisée) à chaque étape :
    :::

    ```{r,teacher=correct}
    set.seed(4321)
    prev <- data.frame(matrix(0,nrow=nrow(Hitters),ncol=3))
    names(prev) <- c("lin","PCR","PLS")
    for (k in 1:10){
    #  print(k)
      ind.test <- bloc==k
      dapp <- Hitters[!ind.test,]
      dtest <- Hitters[ind.test,]
      choix.pcr <- pcr(Salary~.,data=dapp,validation="CV")
      ncomp.pcr <- which.min(choix.pcr$validation$PRESS)
      choix.pls <- plsr(Salary~.,data=dapp,validation="CV")
      ncomp.pls <- which.min(choix.pls$validation$PRESS)
      mod.lin <- lm(Salary~.,data=dapp)
      prev[ind.test,] <- data.frame(
        lin=predict(mod.lin,newdata=dtest),
        PCR=as.vector(predict(choix.pcr,newdata = dtest,ncomp=ncomp.pcr)),
        PLS=as.vector(predict(choix.pls,newdata = dtest,ncomp=ncomp.pls)))
    }
    ```

    ```{r,teacher=correct}
    prev |> mutate(obs=Hitters$Salary) |> 
      summarize_at(1:3,~(mean((.-obs)^2))) |> sqrt()
    ```

    ::: {.corR data-latex=""}
    On compare à un modèle qui prédit toujours la moyenne :
    :::

    ```{r,teacher=correct}
    var(Hitters$Salary) |> sqrt()
    ```

    ::: {.corR data-latex=""}
    On peut retenter l'analyse en considérant toutes les interactions d'ordre 2 :
    :::

    ```{r CV-Hitters,teacher=correct,cache=TRUE,indent='    '}
    set.seed(54321)
    prev1 <- data.frame(matrix(0,nrow=nrow(Hitters),ncol=3))
    names(prev1) <- c("lin","PCR","PLS")
    for (k in 1:10){
    #  print(k)
      ind.test <- bloc==k
      dapp <- Hitters[!ind.test,]
      dtest <- Hitters[ind.test,]
      choix.pcr <- pcr(Salary~.^2,data=dapp,validation="CV")
      ncomp.pcr <- which.min(choix.pcr$validation$PRESS)
      choix.pls <- plsr(Salary~.^2,data=dapp,validation="CV")
      ncomp.pls <- which.min(choix.pls$validation$PRESS)
      mod.lin <- lm(Salary~.^2,data=dapp)
      prev1[ind.test,] <- data.frame(
        lin=predict(mod.lin,newdata=dtest),
        PCR=as.vector(predict(choix.pcr,newdata = dtest,ncomp=ncomp.pcr)),
        PLS=as.vector(predict(choix.pls,newdata = dtest,ncomp=ncomp.pls))
      )
    }
    ```

    ::: {.corR data-latex=""}
    On obtient les performances suivantes :
    :::

    ```{r,teacher=correct}
    prev1 |> mutate(obs=Hitters$Salary) |> 
      summarize_at(1:3,~(mean((.-obs)^2))) |> sqrt()
    ```

    ::: {.corR data-latex=""}
    On mesure bien l'intérêt de réduire la dimension dans ce nouveau contexte.
    :::
